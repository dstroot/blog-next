---
title: 'Microservices: A Technical Solution To A People Problem'
excerpt: 'Microservices mostly exist because teams want to make their own choices, dislike code review or overbearing ‘architects’ above them, and to a lesser extent want to use different and newer languages. This feels good, but...'
coverImage: /assets/blog/img/switchboard.jpg
date: '2022-03-20'
published: false
author:
  name: Dan Stroot
  picture: /assets/blog/authors/dan.jpeg
ogImage:
  url: /assets/blog/img/switchboard.png
seoURL:
---

### A Simple Definition

A microservice is a small, loosely coupled, distributed service. Each service is part of a broader microservices architecture, comprising a set of loosely coupled microservices that operate together to solve a common goal. A collection of microservices can be regarded as a system.

### Why use Microservices?

If microservices architectures were adopted for very specific reasons/needs such as scalability, modularity, or separation of concerns then the architecture could be justified, even in light of the additional operational complexity. However, microservices mostly exist because teams want to make their own choices, dislike code review or overbearing ‘architects’ above them, and to a lesser extent want to use different and newer languages. This feels good! While some people may realize there is nothing smart about this architecture choice for the whole system, many developers are content to care only about thier team -- and not the company or stack as a whole. For them autonomy is nicer than the alternative.

"Doing microservices" is validation for organizations that lack conviction, or a thesis, about how their architecture _should_ be. This ecology of interacting parties, each acting in their own interest for the common good not only sounds utopian, but speaks to an underlying, tacit belief that the emergent mesh of services will approximate the latent natural architecture of the domain. The backlash against microservices is partly because that simply isn't true - instead microservices approximate the natural contours of the technology organization's **organizational model**, not the domain.

Some organizations say that Microservices allows them to release inter-team friction and break inertia by letting each team pretend the system is small and elegant, but this is an illusion - of course, the _overall_ architecture is doing the opposite. They pretend they went "cloud native" and adopted cloud architectures, but it’s often clearer to the truth that they are just letting things happen, and it is very hard to go back. I can’t think of any reasons to why we got here as an industry other than we didn’t want to communicate as people. Making our computer architectures from well-architected simple things into a spider web of organic servces that "just happened" is not something we would do on purpose. Right?

### The Problem

I get it though. Frankly, centralized architecture and standards, when put in the hands of the _wrong_ few could be very counterproductive and feel unfair to everyone else that enjoys software design and doesn’t get to play in the sandbox. This is undeniable. However, isn’t this an argument for smaller teams and less engineering bloat, and better hiring instead? Good architects and technical leaders can keep code maintainable and ensure it remains modular, enabling efficient future developments. These are **not** things we want to give up.

- If we think about resilient systems, the most resilient systems are the ones with the least number of moving parts. The same is true for the fastest systems.

- If we think about deployment and management and upgrades, the simplest systems to deploy and maintain also have the least number of moving parts.

Microservices add complexity and leave us with inefficient communications, less error detection in development, easier ways to introduce errors, a worse debugging experience relying on searching further through distributed log analysis tools, systems that crash more often, and require more and more management-software to deploy. It is a lot easier to debug the interactions between components of a system when they communicate within the same process, especially when one component simply calls a method on another. This is usually just a matter of attaching a debugger to the process, stepping through the method calls and watching variables. There is no straightforward equivalent of this in microservices. Service communications are notoriously difficult to trace and piece together, requiring additional tooling, infrastructure and complexity.

>“we replaced our monolith with microservices so every outage could be like a murder mystery”. 

On the communications front, internal web services are often doubly inefficient by using REST rather than binary transmissions. There’s no reason for any of this, and if multiple microservices hops are used, this all adds up and slows down the system. Even just a conversion to JSON and back is a wasted effort, more so if done dozens of times.

Microservices also risk security problems by introducing more remote access points or chances that service teams use outdated and different library versions.

With all of this, 



Pretty soon you end up with 8 different datastores and things duplicated in weird places, and soon after you don’t know what you have at all.

Microservices allows you to keep adding staff without suffering the consequences of what you have added. If it was hard to manage the monolith and keep adding to it, maybe it was keeping you at the right size until you got your work figured out.

Maybe you should be subtracting and improving, and not just adding. Microservices allows us to break from that inertia by letting each team pretend the system is small and elegant, but this is an illusion - it is growing towards the opposite.

### An Elegant Weapon For A More Civilized Age

While microservices talk likes to pretend the solution is some horrific “monolith”, we never really had “monoliths” before in development that I experienced. What we had were some kinds of tiered architectures. The number of which does not really matter, but in a world of 200 microservices, instead imagine that there are only maybe a few.

Web requests can be managed by one type of instance, that results in one EC2 image or whatever. Anything that can be handled within the lifecycle of one request can be handled there, and these instances are horizontally scaled behind a load balancer.

Asynchronous tasks are managed by a service tier, often connected to a message bus. There may be one of these for each programming language, or maybe a few more, and that would be ok. But because these are horizontally scaled and only ask for new jobs when they have free compute resources, one type of instance can contain code for any manner of asynchronous jobs.

Code that needs to be shared between the asynchronous services and the web tier should be kept in libraries used by both of them and is not a service call.

Errors are eliminated because they can be caught better at compile time and with unit tests.

When changes are made, they can be made in a library, and if the API of a function changes, it is impossible to build/deploy the code until it is fixed.

Do we have to release often? No, we do not, because the release process almost never changes and there are so few components.

Do we need to keep up to date with the latest in container wrangling software, upgrading it and using various service discovery, ingress solutions, service meshes, or otherwise? Also, we do not. These are problems created by overcomplicated architectures. The service tiers can communicate over a message bus and there is no need to know the address of any server.

### What Happened

So people wanted choice, to be able to pick whatever library they wanted, and so on - and for some, this was the path of least resistance. It was new, and novel, and it felt like minimalism - but it wasn’t. Minimalism is about elegance, clean architectures that are easy to draw and explain, and the right number of moving parts.

#### References

- [This Weekly Meeting Took Up 300,000 Hours a Year](https://hbr.org/2014/04/how-a-weekly-meeting-took-up-300000-hours-a-year)
- [How to Avoid Collaboration Fatigue](https://hbr.org/2014/07/how-to-avoid-collaboration-fatigue)

---

Image Credit: **Long Distance Operators, Omaha, 1959**

<span className="text-sm">
Switchboard operators were expected to be courteous, quick-thinking and patient under pressure. They handled all kinds of requests, from providing the time of day to more delicate matters. Reliability engineers who manage microservices are a bit like switchboard operators -- they have to be able to understand all the connections across all the services and what happens if a connection goes down. 
</span>

